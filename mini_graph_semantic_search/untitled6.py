# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qQJZ_5kRt7rdlde78nijuP40w79lFOh5
"""

# -*- coding: utf-8 -*-
"""
build_keyword_graph.py
----------------------
Φτιάχνει γράφο (k-NN) πάνω στα keywords από FAISS + meta.parquet.

Είσοδος:
- vector_index_keywords_only/hnsw.index
- vector_index_keywords_only/meta.parquet
  (στήλες: doc_id, campaign_title (ή doc), ...)

Έξοδοι (στο --outdir):
- nodes.csv
- edges.csv
- graph.gexf
- graph.graphml

Προαπαιτούμενα:
pip install faiss-cpu numpy pandas networkx python-dotenv openai>=1.0.0
"""

import os
from pathlib import Path
import argparse
import numpy as np
import pandas as pd
import networkx as nx

# Προσπάθησε FAISS
HAS_FAISS = True
try:
    import faiss
except Exception:
    HAS_FAISS = False

# Fallback embeddings αν ΔΕΝ υπάρχει FAISS index
from dotenv import load_dotenv
load_dotenv()
from typing import List
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
EMBED_MODEL    = os.getenv("EMBED_MODEL", "text-embedding-3-large")


# ======== DEFAULT CONFIG ========
INDEX_DIR  = Path("vector_index_keywords_only")
INDEX_PATH = INDEX_DIR / "hnsw.index"
META_PATH  = INDEX_DIR / "meta.parquet"

TITLE_COL  = "campaign_title"  # hover/label field
K_NEIGHBORS = 10               # k για k-NN
LIMIT_N     = 0                # 0 = όλα, αλλιώς δειγματοληψία έως N
SEED        = 42
NORMALIZE   = True             # normalizing → cosine = inner product


# ======== HELPERS ========
def normalize_rows(x: np.ndarray) -> np.ndarray:
    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12
    return x / n

def load_meta(meta_path: Path, title_col: str) -> pd.DataFrame:
    if not meta_path.exists():
        raise FileNotFoundError(f"meta.parquet not found at {meta_path}")
    meta = pd.read_parquet(meta_path)
    if title_col not in meta.columns:
        if "doc" in meta.columns:
            title_col = "doc"
        elif "keyword" in meta.columns:
            title_col = "keyword"
        else:
            title_col = "doc_id"
    return meta, title_col

def reconstruct_vectors_from_faiss(index_path: Path, n_wanted: int) -> np.ndarray:
    index = faiss.read_index(str(index_path))
    n_total = index.ntotal
    n = min(n_total, n_wanted) if n_wanted > 0 else n_total

    D = index.d
    vecs = np.zeros((n, D), dtype="float32")
    tmp  = np.zeros((D,), dtype="float32")
    for i in range(n):
        # IndexHNSWFlat υποστηρίζει reconstruct
        index.reconstruct(int(i), tmp)
        vecs[i] = tmp
    return vecs

def embed_texts_openai(texts: List[str]) -> np.ndarray:
    if not OPENAI_API_KEY:
        raise RuntimeError("Λείπει OPENAI_API_KEY για fallback embeddings.")
    from openai import OpenAI
    client = OpenAI(api_key=OPENAI_API_KEY)
    embs = []
    BATCH = 128
    for i in range(0, len(texts), BATCH):
        chunk = texts[i:i+BATCH]
        resp = client.embeddings.create(model=EMBED_MODEL, input=chunk)
        embs.extend([e.embedding for e in resp.data])
    X = np.array(embs, dtype="float32")
    return X

def compute_knn_from_faiss(index_path: Path, queries: np.ndarray, k: int):
    index = faiss.read_index(str(index_path))
    # Αν το index έχει λιγότερα σημεία από queries (λόγω LIMIT_N), πάρε top-k από το ίδιο subset
    # Παρόλα αυτά, FAISS index είναι global· εδώ στέλνουμε τα query vectors του subset.
    D, I = index.search(queries, k)
    return D, I

def compute_knn_cpu(vecs: np.ndarray, k: int):
    # Pure NumPy cosine similarities (αποφεύγουμε scikit για να κρατήσουμε λίγες εξαρτήσεις)
    # vecs assumed normalized
    sims = vecs @ vecs.T  # (n x n)
    # Για κάθε i, κρατάμε τα top-k indices (συμπεριλ. του εαυτού του)
    n = sims.shape[0]
    I = np.argpartition(-sims, kth=min(k-1, n-1), axis=1)[:, :k]  # unsorted top-k
    # ταξινόμηση αυτών των top-k
    row_indices = np.arange(n)[:, None]
    sorted_order = np.argsort(-sims[row_indices, I], axis=1)
    I_sorted = I[row_indices, sorted_order]
    D_sorted = sims[row_indices, I_sorted]
    return D_sorted, I_sorted


# ======== MAIN BUILD ========
def build_graph(index_path: Path,
                meta_path: Path,
                outdir: Path,
                title_col: str = TITLE_COL,
                k: int = K_NEIGHBORS,
                limit_n: int = LIMIT_N,
                seed: int = SEED,
                normalize: bool = NORMALIZE):
    outdir.mkdir(parents=True, exist_ok=True)

    # 1) Διάβασε meta
    meta, title_col = load_meta(meta_path, title_col)
    n_total = len(meta)
    if n_total == 0:
        raise ValueError("Empty meta.parquet.")

    # 2) Δείγμα (προαιρετικό)
    if limit_n and limit_n > 0 and limit_n < n_total:
        rng = np.random.default_rng(seed)
        pick = np.sort(rng.choice(n_total, size=limit_n, replace=False))
        meta_sub = meta.iloc[pick].reset_index(drop=True)
        idx_map = pick  # global -> local mapping
    else:
        meta_sub = meta.reset_index(drop=True)
        idx_map = np.arange(n_total)

    n = len(meta_sub)

    # 3) Πάρε vectors
    if HAS_FAISS and index_path.exists():
        vecs = reconstruct_vectors_from_faiss(index_path, n_wanted=n)
        # reconstruct() διαβάζει με την global σειρά (0..ntotal-1).
        # Αν έχουμε pick/limit: έχουμε ήδη περιορίσει n ώστε να ταιριάζει.
        # NOTE: Αν θες vectors ΜΟΝΟ για τα selected indices (π.χ. arbitrary pick),
        # θα χρειαστείς reconstruct με τους πραγματικούς global IDs (πιο περίπλοκο).
    else:
        # Fallback: embeddings μόνο για τα labels (λιγότερο ακριβές από τα doc vectors)
        label_series = None
        for c in [title_col, "keyword", "doc", "campaign_title"]:
            if c in meta_sub.columns:
                label_series = meta_sub[c].astype(str).fillna("")
                break
        if label_series is None:
            label_series = meta_sub["doc_id"].astype(str).fillna("")
        vecs = embed_texts_openai(label_series.tolist())

    if normalize:
        vecs = normalize_rows(vecs)

    # 4) Υπολόγισε k-NN
    if HAS_FAISS and index_path.exists():
        # αναζήτηση στο FAISS index (μπορεί να επιστρέψει neighbors εκτός subset)
        D, I = compute_knn_from_faiss(index_path, vecs, k)
        # Φτιάξε mapping από global -> local όπου γίνεται (αν δουλεύεις με subset/pick)
        if len(idx_map) == len(vecs) and (idx_map == np.arange(len(vecs))).all():
            # full case, global ids == local ids
            pass
        else:
            # Αν έχεις πραγματικό subset (όχι απλά range), θα χρειαστεί reconstruct με συγκεκριμένα IDs.
            # Για απλότητα: κρατάμε μόνο edges που μπαίνουν στο current subset
            g2l = {int(g): i for i, g in enumerate(idx_map)}
            I_local = np.full_like(I, fill_value=-1)
            for i in range(I.shape[0]):
                for j in range(I.shape[1]):
                    I_local[i, j] = g2l.get(int(I[i, j]), -1)
            I = I_local
            # Πετάμε neighbors -1 (εκτός subset) αντικαθιστώντας με self
            I[I < 0] = np.arange(I.shape[0])[:, None]
    else:
        D, I = compute_knn_cpu(vecs, k)

    # 5) Χτίσε γράφο (undirected)
    G = nx.Graph()
    # nodes
    for i in range(n):
        row = meta_sub.iloc[i]
        G.add_node(i,
                   doc_id=str(row.get("doc_id", i)),
                   title=str(row.get(title_col, "")),
                   **{c: (None if pd.isna(row[c]) else row[c]) for c in meta_sub.columns if c not in ("doc_id", title_col)})
    # edges (βάζουμε weight = similarity)
    for i in range(n):
        for j_idx in range(k):
            j = int(I[i, j_idx])
            if j == i:  # skip self
                continue
            w = float(D[i, j_idx])
            a, b = (i, j) if i < j else (j, i)
            if G.has_edge(a, b):
                # κράτα το μεγαλύτερο weight (συμμετρικό)
                if w > G[a][b].get("weight", -1):
                    G[a][b]["weight"] = w
            else:
                G.add_edge(a, b, weight=w)

    # 6) Αποθήκευση
    out_nodes = outdir / "nodes.csv"
    out_edges = outdir / "edges.csv"
    out_gexf  = outdir / "graph.gexf"
    out_gml   = outdir / "graph.graphml"

    # nodes.csv
    nodes_df = pd.DataFrame([
        {"id": n, "doc_id": G.nodes[n].get("doc_id", n), "title": G.nodes[n].get("title", "")}
        | {k: v for k, v in G.nodes[n].items() if k not in ("doc_id", "title")}
        for n in G.nodes()
    ])
    nodes_df.to_csv(out_nodes, index=False, encoding="utf-8")

    # edges.csv
    edges_df = pd.DataFrame([
        {"source": u, "target": v, "weight": d.get("weight", 1.0)}
        for u, v, d in G.edges(data=True)
    ])
    edges_df.to_csv(out_edges, index=False, encoding="utf-8")

    # GEXF / GraphML
    nx.write_gexf(G, out_gexf)
    try:
        nx.write_graphml(G, out_gml)
    except Exception as e:
        print(f"GraphML save error (skip): {e}")

    print(f"OK: saved nodes → {out_nodes.resolve()}")
    print(f"OK: saved edges → {out_edges.resolve()}")
    print(f"OK: saved GEXF  → {out_gexf.resolve()}")
    print(f"OK: saved GraphML → {out_gml.resolve()}")
    print(f"Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}")


# ======== CLI ========
def parse_args():
    p = argparse.ArgumentParser(description="Φτιάξε k-NN γράφο από FAISS + meta.parquet.")
    p.add_argument("--index-dir", default=str(INDEX_DIR), help="Φάκελος index/meta (default: vector_index_keywords_only)")
    p.add_argument("--k", type=int, default=K_NEIGHBORS, help=f"Neighbors per node (default: {K_NEIGHBORS})")
    p.add_argument("--limit", type=int, default=LIMIT_N, help="Μέγιστος αριθμός nodes (0=όλα)")
    p.add_argument("--title-col", default=TITLE_COL, help=f"Στήλη τίτλου/label (default: {TITLE_COL})")
    p.add_argument("--outdir", default="graph_out", help="Φάκελος εξόδου (default: graph_out)")
    p.add_argument("--no-normalize", action="store_true", help="Μην κάνεις L2 normalize τα vectors")
    return p.parse_args()

def main():
    args = parse_args()
    index_dir = Path(args.index_dir)
    index_path = index_dir / "hnsw.index"
    meta_path  = index_dir / "meta.parquet"
    outdir     = Path(args.outdir)

    build_graph(index_path=index_path,
                meta_path=meta_path,
                outdir=outdir,
                title_col=args.title_col,
                k=args.k,
                limit_n=args.limit,
                seed=SEED,
                normalize=not args.no_normalize)

if __name__ == "__main__":
    main()