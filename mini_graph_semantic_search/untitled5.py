# -*- coding: utf-8 -*-
"""
semantic_search_oneflow.py
--------------------------------
Î•Î½Î¹Î±Î¯Î± ÎµÏ†Î±ÏÎ¼Î¿Î³Î® semantic search Ï€Î¬Î½Ï‰ ÏƒÎµ Î¼Î¹ÎºÏÏŒ keyword graph Î¼Îµ FAISS + OpenAI.

Flow ÎµÎºÎºÎ¯Î½Î·ÏƒÎ·Ï‚:
1) Î–Î·Ï„Î¬ 2 API keys (OPENAI_API_KEY + GEMINI_API_KEY).
2) Î–Î·Ï„Î¬ Î´Î¹Î±Î´ÏÎ¿Î¼Î® CSV (ÏƒÏ„Î®Î»Î· 'keyword' Î® 1Î· ÏƒÏ„Î®Î»Î·).
3) Î§Ï„Î¯Î¶ÎµÎ¹/Î¾Î±Î½Î±Ï‡Ï„Î¯Î¶ÎµÎ¹ index (FAISS HNSW).
4) Loop: Î¿ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ Î´Î¯Î½ÎµÎ¹ Î¦Î¡Î‘Î£Î— Î® ÎŸÎ›ÎŸÎšÎ›Î—Î¡ÎŸ Î‘Î¡Î˜Î¡ÎŸ Î® PATH ÏƒÎµ .txt,
   ÎºÎ±Î¹ ÎµÏ€Î¹Î»Î­Î³ÎµÎ¹ Î±Î½ Î¸Î­Î»ÎµÎ¹ GPT-4o(-mini) Ï€ÎµÏÎ¯Î»Î·ÏˆÎ· Ï€ÏÎ¹Î½ Ï„Î¿ semantic search.

Î‘Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚:
pip install faiss-cpu numpy pandas python-dotenv openai>=1.0.0 rapidfuzz
"""

import os, re, uuid, getpass
from pathlib import Path
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from openai import OpenAI
import faiss
from rapidfuzz import fuzz

# ================== Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ ==================
load_dotenv()

EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-large")
DIM = 3072
NORMALIZE = True
BATCH = 128

INDEX_DIR = Path("vector_index")
INDEX_PATH = INDEX_DIR / "hnsw.index"
META_PATH  = INDEX_DIR / "meta.parquet"

# ---- Î”Î•Î¥Î¤Î•Î¡ÎŸ API KEY: GEMINI ----
GEMINI_API_KEY_NAME = "GEMINI_API_KEY"  # env var name Î³Î¹Î± Gemini


# ================== Î’ÎŸÎ—Î˜Î—Î¤Î™ÎšÎ‘ ==================
def clean_text(t: str) -> str:
    return re.sub(r"\s+", " ", str(t)).strip()

def normalize_rows(x: np.ndarray) -> np.ndarray:
    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12
    return x / n

def ask_yesno(msg: str, default_yes=True) -> bool:
    suffix = "[Y/n]" if default_yes else "[y/N]"
    ans = input(f"{msg} {suffix} ").strip().lower()
    if not ans:
        return default_yes
    return ans in ("y", "yes", "nai", "Î½Î±Î¹")

def summarize_with_gpt(text: str, model: str = "gpt-4o-mini", api_key: str = "") -> str:
    """
    Î£ÏÎ½Ï„Î¿Î¼Î· Ï€ÎµÏÎ¯Î»Î·ÏˆÎ· (<=80 Î»Î­Î¾ÎµÎ¹Ï‚) Î³Î¹Î± semantic search.
    Î’Î¬Î»Îµ model="gpt-4o" Î±Î½ Î¸Î­Î»ÎµÎ¹Ï‚ full 4o.
    """
    try:
        client = OpenAI(api_key=api_key)
        resp = client.chat.completions.create(
            model=model,
            messages=[{
                "role": "user",
                "content": (
                    "Î”ÏÏƒÎµ Ï€Î¿Î»Ï ÏƒÏÎ½Ï„Î¿Î¼Î· Ï€ÎµÏÎ¯Î»Î·ÏˆÎ· (<=80 Î»Î­Î¾ÎµÎ¹Ï‚) "
                    "Î¼Îµ Ï„Î¹Ï‚ Î²Î±ÏƒÎ¹ÎºÎ­Ï‚ Î­Î½Î½Î¿Î¹ÎµÏ‚ Î³Î¹Î± semantic search:\n\n" + text
                )
            }],
            temperature=0.0
        )
        summ = (resp.choices[0].message.content or "").strip()
        return summ if summ else text
    except Exception as e:
        print(f"!! Î‘Ï€Î­Ï„Ï…Ï‡Îµ Ï„Î¿ summarization ({e}). Î£Ï…Î½ÎµÏ‡Î¯Î¶Ï‰ Î¼Îµ Î±ÏÏ‡Î¹ÎºÏŒ ÎºÎµÎ¯Î¼ÎµÎ½Î¿.")
        return text

def embed_texts(client, texts):
    embs = []
    for i in range(0, len(texts), BATCH):
        chunk = texts[i:i+BATCH]
        resp = client.embeddings.create(model=EMBED_MODEL, input=chunk)
        embs.extend([e.embedding for e in resp.data])
    X = np.array(embs, dtype="float32")
    return normalize_rows(X) if NORMALIZE else X

def embed_one(client, text: str) -> np.ndarray:
    resp = client.embeddings.create(model=EMBED_MODEL, input=[text])
    v = np.array([resp.data[0].embedding], dtype="float32")
    return normalize_rows(v) if NORMALIZE else v

def read_keywords_csv(path: Path, column: str | None = None, columns: str | None = None) -> list[str]:
    """
    Î¥Ï€Î¿ÏƒÏ„Î·ÏÎ¯Î¶ÎµÎ¹:
    - 1 ÏƒÏ„Î®Î»Î·: --column <name> (Î® Î±Î½ Î»ÎµÎ¯Ï€ÎµÎ¹, Ï€Î±Î¯ÏÎ½ÎµÎ¹ 'keyword' Î® Ï„Î·Î½ 1Î· ÏƒÏ„Î®Î»Î·)
    - Î Î¿Î»Î»Î­Ï‚ ÏƒÏ„Î®Î»ÎµÏ‚: columns="col1,col2,..." (ÎµÎ½ÏÎ½ÎµÎ¹ ÏŒÎ»ÎµÏ‚ Ï„Î¹Ï‚ Ï„Î¹Î¼Î­Ï‚)
    ÎšÎ¬Î¸Îµ ÎºÎµÎ»Î¯ => Î­Î½Î± keyword (trim), Î±Ï†Î±Î¹ÏÎµÎ¯ Î´Î¹Ï€Î»ÏŒÏ„Ï…Ï€Î±/ÎºÎµÎ½Î¬, ÎºÏÎ±Ï„Î¬ ÏƒÎµÎ¹ÏÎ¬.
    """
    df = pd.read_csv(path)

    def clean_series(s: pd.Series) -> list[str]:
        seen, out = set(), []
        for x in s.astype(str):
            t = clean_text(x)
            if t and t not in seen:
                seen.add(t); out.append(t)
        return out

    if columns:
        cols = [c.strip() for c in columns.split(",") if c.strip()]
        missing = [c for c in cols if c not in df.columns]
        if missing:
            raise ValueError(f"Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÏƒÏ„Î®Î»ÎµÏ‚: {missing}")
        combined = []
        for c in cols:
            combined.extend(clean_series(df[c]))
        # Î¼Î¿Î½Î±Î´Î¹ÎºÎ¬ Î¼Îµ Î´Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· ÏƒÎµÎ¹ÏÎ¬Ï‚
        seen, out = set(), []
        for t in combined:
            if t not in seen:
                seen.add(t); out.append(t)
        return out

    if column:
        if column not in df.columns:
            raise ValueError(f"Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÏƒÏ„Î®Î»Î· '{column}' ÏƒÏ„Î¿ CSV.")
        return clean_series(df[column])

    if "keyword" in df.columns:
        return clean_series(df["keyword"])
    return clean_series(df.iloc[:, 0])

def read_maybe_file(user_input: str) -> str:
    """
    Î‘Î½ Î¿ Ï‡ÏÎ®ÏƒÏ„Î·Ï‚ Î­Î´Ï‰ÏƒÎµ path ÏƒÎµ .txt Ï€Î¿Ï… Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ -> Î´Î¹Î¬Î²Î±ÏƒÎµ Î±ÏÏ‡ÎµÎ¯Î¿.
    Î‘Î»Î»Î¹ÏÏ‚, ÎµÏ€Î­ÏƒÏ„ÏÎµÏˆÎµ Ï„Î·Î½ Î¯Î´Î¹Î± Ï„Î· Ï†ÏÎ¬ÏƒÎ·/Î¬ÏÎ¸ÏÎ¿ Ï€Î¿Ï… ÎµÏ€Î¹ÎºÏŒÎ»Î»Î·ÏƒÎµ.
    """
    p = Path(user_input.strip().strip('"').strip("'"))
    if p.suffix.lower() == ".txt" and p.exists():
        return p.read_text(encoding="utf-8")
    return user_input


# ================== INDEX OPS ==================
def build_index_from_keywords(keywords: list[str], openai_key: str):
    if not keywords:
        raise ValueError("Î†Î´ÎµÎ¹Î± Î»Î¯ÏƒÏ„Î± keywords.")
    INDEX_DIR.mkdir(exist_ok=True, parents=True)

    client = OpenAI(api_key=openai_key)
    X = embed_texts(client, keywords)

    idx = faiss.IndexHNSWFlat(DIM, 64, faiss.METRIC_INNER_PRODUCT)
    idx.hnsw.efConstruction = 200
    idx.hnsw.efSearch = 64
    idx.add(X)

    meta = pd.DataFrame({
        "doc_id": [str(uuid.uuid4()) for _ in keywords],
        "keyword": keywords
    })
    faiss.write_index(idx, str(INDEX_PATH))
    meta.to_parquet(META_PATH, index=False)
    print(f"OK: Î¦Ï„Î¹Î¬Ï‡Ï„Î·ÎºÎµ index Î¼Îµ {idx.ntotal} keywords.")

def load_index_and_meta():
    if not (INDEX_PATH.exists() and META_PATH.exists()):
        raise RuntimeError("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ index. Î§Ï„Î¯ÏƒÎµ Ï„Î¿Î½ Ï€ÏÏÏ„Î±.")
    idx = faiss.read_index(str(INDEX_PATH))
    meta = pd.read_parquet(META_PATH)
    return idx, meta


# ================== Î•ÎšÎšÎ™ÎÎ—Î£Î— (Credentials + CSV) ==================
def interactive_collect_credentials_and_csv():
    print("=== Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Î•ÎºÎºÎ¯Î½Î·ÏƒÎ·Ï‚ ===")
    openai_key = os.getenv("OPENAI_API_KEY", "").strip()
    gemini_key  = os.getenv(GEMINI_API_KEY_NAME, "").strip()

    if not openai_key:
        print("Î”ÏÏƒÎµ OPENAI_API_KEY (Î´ÎµÎ½ Î¸Î± Ï†Î±Î¯Î½ÎµÏ„Î±Î¹):")
        openai_key = getpass.getpass("OPENAI_API_KEY: ").strip()
        if not openai_key:
            raise RuntimeError("Î§ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ OPENAI_API_KEY.")

    if not gemini_key:
        print(f"Î”ÏÏƒÎµ {GEMINI_API_KEY_NAME} (ÎºÎ»ÎµÎ¹Î´Î¯ Gemini, Î´ÎµÎ½ Î¸Î± Ï†Î±Î¯Î½ÎµÏ„Î±Î¹):")
        gemini_key = getpass.getpass(f"{GEMINI_API_KEY_NAME}: ").strip()
        if not gemini_key:
            raise RuntimeError(f"Î§ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ {GEMINI_API_KEY_NAME}.")

    # Î’Î¬Î»Îµ Ï„Î± ÎºÎ±Î¹ ÏƒÏ„Î¿ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½ Î³Î¹Î± runtime
    os.environ["OPENAI_API_KEY"] = openai_key
    os.environ[GEMINI_API_KEY_NAME] = gemini_key

    # CSV path
    while True:
        csv_path_str = input("Î”ÏÏƒÎµ Î´Î¹Î±Î´ÏÎ¿Î¼Î® CSV Î¼Îµ keywords (Ï€.Ï‡. data/keywords.csv): ").strip().strip('"').strip("'")
        if not csv_path_str:
            print("Î Î±ÏÎ±ÎºÎ±Î»Ï Î³ÏÎ¬ÏˆÎµ Î´Î¹Î±Î´ÏÎ¿Î¼Î®.")
            continue
        csv_path = Path(csv_path_str)
        if not csv_path.exists():
            print(f"Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Î±ÏÏ‡ÎµÎ¯Î¿: {csv_path}")
            continue
        try:
            # Î‘Î½ Î¸Î­Î»ÎµÎ¹Ï‚ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½ÎµÏ‚ ÏƒÏ„Î®Î»ÎµÏ‚, Ï€ÎµÏ‚ Î¼Î¿Ï… Î½Î± Ï„Î¿ ÏÏ‰Ï„Î¬ÎµÎ¹ ÎµÎ´Ï (column/columns).
            kws = read_keywords_csv(csv_path)
            if not kws:
                print("Î¤Î¿ CSV Ï†Î±Î¯Î½ÎµÏ„Î±Î¹ Î¬Î´ÎµÎ¹Î¿. Î”ÏÏƒÎµ Î¬Î»Î»Î¿ Î±ÏÏ‡ÎµÎ¯Î¿.")
                continue
            return openai_key, gemini_key, csv_path, kws
        except Exception as e:
            print(f"Î£Ï†Î¬Î»Î¼Î± Î±Î½Î¬Î³Î½Ï‰ÏƒÎ·Ï‚ CSV: {e}. Î”ÏÏƒÎµ Î¬Î»Î»Î¿ Î±ÏÏ‡ÎµÎ¯Î¿.")


# ================== APP FLOW ==================
def main():
    # 1) Î–Î®Ï„Î± 2 keys + CSV ÏƒÏ„Î·Î½ Î±ÏÏ‡Î®
    openai_key, gemini_key, csv_path, keywords = interactive_collect_credentials_and_csv()

    print("\nTip: Î£Ï„Î¿ ÎµÏ€ÏŒÎ¼ÎµÎ½Î¿ Î²Î®Î¼Î± Î¼Ï€Î¿ÏÎµÎ¯Ï‚ Î½Î± ÎµÏ€Î¹ÎºÎ¿Î»Î»Î®ÏƒÎµÎ¹Ï‚ Î¿Î»ÏŒÎºÎ»Î·ÏÎ¿ Î¬ÏÎ¸ÏÎ¿ Î® Î±Ï€Î»ÏÏ‚ Î½Î± Î´ÏÏƒÎµÎ¹Ï‚ Î¼Î¹Î± Ï†ÏÎ¬ÏƒÎ·.")
    print("Î‘Î½ Î´ÏÏƒÎµÎ¹Ï‚ path ÏƒÎµ .txt, Î¸Î± Î´Î¹Î±Î²Î¬ÏƒÎµÎ¹ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿.\n")

    # 2) Î§Ï„Î¯ÏƒÎµ index (Î±Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î®Î´Î·, ÏÏÏ„Î±)
    if INDEX_PATH.exists() or META_PATH.exists():
        if ask_yesno("Î¥Ï€Î¬ÏÏ‡ÎµÎ¹ Î®Î´Î· index. Î˜ÎµÏ‚ Î½Î± Ï„Î¿Î½ Î¾Î±Î½Î±Ï‡Ï„Î¯ÏƒÏ‰ Î±Ï€ÏŒ Ï„Î¿ CSV;", default_yes=False):
            build_index_from_keywords(keywords, openai_key)
    else:
        build_index_from_keywords(keywords, openai_key)

    # 3) Î•Î½Î¹Î±Î¯Î¿ flow: Î´ÏÏƒÎµ Î¦Î¡Î‘Î£Î— Î® Î‘Î¡Î˜Î¡ÎŸ Î® PATH ÏƒÎµ .txt
    while True:
        print("\n=== Semantic Search ===")
        user_in = input("Î“ÏÎ¬ÏˆÎµ Ï†ÏÎ¬ÏƒÎ·/ÎµÏ€Î¹ÎºÏŒÎ»Î»Î·ÏƒÎµ Î¬ÏÎ¸ÏÎ¿ Î® Î´ÏÏƒÎµ path ÏƒÎµ .txt (Enter Î³Î¹Î± Î­Î¾Î¿Î´Î¿): ").strip()
        if not user_in:
            print("ÎˆÎ¾Î¿Î´Î¿Ï‚. ğŸ‘‹")
            break

        try:
            raw_text = read_maybe_file(user_in)

            # Î¡ÏÏ„Î± Î±Î½ Î¸Î­Î»ÎµÎ¹ GPT-4o Ï€ÎµÏÎ¯Î»Î·ÏˆÎ· Ï€ÏÎ¹Î½ Ï„Î¿ embedding
            if ask_yesno("Î˜ÎµÏ‚ GPT-4o Ï€ÎµÏÎ¯Î»Î·ÏˆÎ· Ï€ÏÎ¹Î½ Ï„Î¿ semantic search;", default_yes=True):
                use_full = ask_yesno("ÎÎ± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®ÏƒÏ‰ Ï„Î¿ gpt-4o (ÏŒÏ‡Î¹ mini);", default_yes=False)
                model = "gpt-4o" if use_full else "gpt-4o-mini"
                text_for_search = summarize_with_gpt(raw_text, model=model, api_key=openai_key)
                print("\n--- Î ÎµÏÎ¯Î»Î·ÏˆÎ· Ï€Î¿Ï… Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î·Î¸ÎµÎ¯ Î³Î¹Î± embedding ---")
                print(text_for_search[:800], "...\n" if len(text_for_search)>800 else "\n")
            else:
                text_for_search = raw_text

            # Semantic search
            client = OpenAI(api_key=openai_key)
            idx, meta = load_index_and_meta()
            qv = embed_one(client, clean_text(text_for_search))
            k = 10
            topn = min(max(k*5, k), len(meta))
            D, I = idx.search(qv, topn)

            cand = meta.iloc[I[0]].copy()
            cand["score_vec"] = D[0]
            cand["keyword_boost"] = cand["keyword"].fillna("").apply(
                lambda t: fuzz.token_set_ratio(text_for_search, str(t))/100.0
            )
            cand["final_score"] = 0.9*cand["score_vec"] + 0.1*cand["keyword_boost"]

            res = cand.sort_values("final_score", ascending=False).head(k)[["keyword","final_score"]]
            print("\nÎ‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±:")
            print(res.to_string(index=False))

            # Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ® Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
            if ask_yesno("ÎÎ± Î±Ï€Î¿Î¸Î·ÎºÎµÏÏƒÏ‰ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± ÏƒÎµ CSV;", default_yes=False):
                out_path = Path("semantic_results.csv")
                res.to_csv(out_path, index=False, encoding="utf-8")
                print(f"OK: Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎ±Î½ ÏƒÏ„Î¿ {out_path.resolve()}")

        except Exception as e:
            print(f"Î£Ï†Î¬Î»Î¼Î±: {e}")


if __name__ == "__main__":
    main()
