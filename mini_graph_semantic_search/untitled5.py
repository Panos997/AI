# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xAjTaYiOZt8hIAcb6M-ogmSjRTNjU81U
"""

import os, re, uuid, getpass
from pathlib import Path
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from openai import OpenAI
import faiss
from rapidfuzz import fuzz

# ================== Î’Î‘Î£Î™ÎšÎ•Î£ Î¡Î¥Î˜ÎœÎ™Î£Î•Î™Î£ ==================
load_dotenv()

EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-large")
DIM = 3072
NORMALIZE = True
BATCH = 128

INDEX_DIR = Path("vector_index")
INDEX_PATH = INDEX_DIR / "hnsw.index"
META_PATH  = INDEX_DIR / "meta.parquet"

# ÎœÏ€Î¿ÏÎµÎ¯Ï‚ Î½Î± Î±Î»Î»Î¬Î¾ÎµÎ¹Ï‚ Ï„Î¿ ÏŒÎ½Î¿Î¼Î± Ï„Î¿Ï… 2Î¿Ï… API key ÎµÎ´Ï:
OTHER_API_KEY_NAME = "OTHER_API_KEY"

# ================== HELPERS ==================
def clean_text(t: str) -> str:
    return re.sub(r"\s+", " ", str(t)).strip()

def normalize_rows(x: np.ndarray) -> np.ndarray:
    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12
    return x / n

def embed_texts(client, texts):
    embs = []
    for i in range(0, len(texts), BATCH):
        chunk = texts[i:i+BATCH]
        resp = client.embeddings.create(model=EMBED_MODEL, input=chunk)
        embs.extend([e.embedding for e in resp.data])
    X = np.array(embs, dtype="float32")
    return normalize_rows(X) if NORMALIZE else X

def embed_one(client, text: str) -> np.ndarray:
    resp = client.embeddings.create(model=EMBED_MODEL, input=[text])
    v = np.array([resp.data[0].embedding], dtype="float32")
    return normalize_rows(v) if NORMALIZE else v

def read_keywords_csv(path: Path) -> list[str]:
    df = pd.read_csv(path)
    col = "keyword" if "keyword" in df.columns else df.columns[0]
    s = df[col].astype(str)
    seen, out = set(), []
    for x in s:
        x = clean_text(x)
        if x and x not in seen:
            seen.add(x); out.append(x)
    return out

def save_to_env(pairs: dict[str, str], env_path: Path = Path(".env")):
    existing = {}
    if env_path.exists():
        for line in env_path.read_text(encoding="utf-8").splitlines():
            if "=" in line and not line.strip().startswith("#"):
                k, v = line.split("=", 1)
                existing[k.strip()] = v.strip()
    existing.update(pairs)
    lines = [f"{k}={v}" for k, v in existing.items()]
    env_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
    print(f">> Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎ±Î½ ÏƒÏ„Î¿ {env_path} (Î½Î± ÎœÎ—Î Î³Î¯Î½ÎµÎ¹ commit).")

def ask_yesno(msg: str, default_yes=True) -> bool:
    suffix = "[Y/n]" if default_yes else "[y/N]"
    ans = input(f"{msg} {suffix} ").strip().lower()
    if not ans:
        return default_yes
    return ans in ("y","yes","nai","Î½Î±Î¹")

# ================== INTERACTIVE START ==================
def interactive_collect_credentials_and_csv():
    print("=== Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ Î•ÎºÎºÎ¯Î½Î·ÏƒÎ·Ï‚ ===")
    openai_key = os.getenv("OPENAI_API_KEY", "").strip()
    other_key  = os.getenv(OTHER_API_KEY_NAME, "").strip()

    if not openai_key:
        print("Î”ÏÏƒÎµ OPENAI_API_KEY (Î´ÎµÎ½ Î¸Î± Ï†Î±Î¯Î½ÎµÏ„Î±Î¹):")
        openai_key = getpass.getpass("OPENAI_API_KEY: ").strip()
        if not openai_key:
            raise RuntimeError("Î§ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ OPENAI_API_KEY.")

    if not other_key:
        print(f"Î”ÏÏƒÎµ {OTHER_API_KEY_NAME} (Î´ÎµÎ½ Î¸Î± Ï†Î±Î¯Î½ÎµÏ„Î±Î¹):")
        other_key = getpass.getpass(f"{OTHER_API_KEY_NAME}: ").strip()
        if not other_key:
            raise RuntimeError(f"Î§ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ {OTHER_API_KEY_NAME}.")

    # Î’Î¬Î»Îµ Ï„Î± ÎºÎ±Î¹ ÏƒÏ„Î¿ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½ Î³Î¹Î± runtime
    os.environ["OPENAI_API_KEY"] = openai_key
    os.environ[OTHER_API_KEY_NAME] = other_key

    # Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬ ÏƒÏÏƒÎµ Ï„Î± ÏƒÏ„Î¿ .env
    if ask_yesno("Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÎºÎ»ÎµÎ¹Î´Î¹ÏÎ½ ÏƒÏ„Î¿ .env;"):
        save_to_env({"OPENAI_API_KEY": openai_key, OTHER_API_KEY_NAME: other_key})

    # Î¡ÏÏ„Î± Î³Î¹Î± CSV (Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÎ¯Î½Î±Î¹ private path)
    while True:
        csv_path_str = input("Î”ÏÏƒÎµ Î´Î¹Î±Î´ÏÎ¿Î¼Î® CSV Î¼Îµ keywords (Ï€.Ï‡. data/keywords.csv): ").strip().strip('"').strip("'")
        if not csv_path_str:
            print("Î Î±ÏÎ±ÎºÎ±Î»Ï Î³ÏÎ¬ÏˆÎµ Î´Î¹Î±Î´ÏÎ¿Î¼Î®.")
            continue
        csv_path = Path(csv_path_str)
        if not csv_path.exists():
            print(f"Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Î±ÏÏ‡ÎµÎ¯Î¿: {csv_path}")
            continue
        try:
            kws = read_keywords_csv(csv_path)
            if not kws:
                print("Î¤Î¿ CSV Ï†Î±Î¯Î½ÎµÏ„Î±Î¹ Î¬Î´ÎµÎ¹Î¿. Î”ÏÏƒÎµ Î¬Î»Î»Î¿ Î±ÏÏ‡ÎµÎ¯Î¿.")
                continue
            return openai_key, other_key, csv_path, kws
        except Exception as e:
            print(f"Î£Ï†Î¬Î»Î¼Î± Î±Î½Î¬Î³Î½Ï‰ÏƒÎ·Ï‚ CSV: {e}. Î”ÏÏƒÎµ Î¬Î»Î»Î¿ Î±ÏÏ‡ÎµÎ¯Î¿.")

# ================== INDEX OPS ==================
def build_index_from_keywords(keywords: list[str], openai_key: str):
    if not keywords:
        raise ValueError("Î†Î´ÎµÎ¹Î± Î»Î¯ÏƒÏ„Î± keywords.")
    INDEX_DIR.mkdir(exist_ok=True, parents=True)

    client = OpenAI(api_key=openai_key)
    X = embed_texts(client, keywords)

    idx = faiss.IndexHNSWFlat(DIM, 64, faiss.METRIC_INNER_PRODUCT)
    idx.hnsw.efConstruction = 200
    idx.hnsw.efSearch = 64
    idx.add(X)

    meta = pd.DataFrame({
        "doc_id": [str(uuid.uuid4()) for _ in keywords],
        "keyword": keywords
    })
    faiss.write_index(idx, str(INDEX_PATH))
    meta.to_parquet(META_PATH, index=False)
    print(f"OK: Î¦Ï„Î¹Î¬Ï‡Ï„Î·ÎºÎµ index Î¼Îµ {idx.ntotal} keywords.")

def load_index_and_meta():
    if not (INDEX_PATH.exists() and META_PATH.exists()):
        raise RuntimeError("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ index. Î§Ï„Î¯ÏƒÎµ Ï„Î¿Î½ Ï€ÏÏÏ„Î±.")
    idx = faiss.read_index(str(INDEX_PATH))
    meta = pd.read_parquet(META_PATH)
    return idx, meta

# ================== SEARCH OPS ==================
def search_by_query(text_query: str, k: int = 10, openai_key: str = ""):
    client = OpenAI(api_key=openai_key)
    idx, meta = load_index_and_meta()

    qv = embed_one(client, clean_text(text_query))
    topn = min(max(k*5, k), len(meta))
    D, I = idx.search(qv, topn)

    cand = meta.iloc[I[0]].copy()
    cand["score_vec"] = D[0]
    cand["keyword_boost"] = cand["keyword"].fillna("").apply(
        lambda t: fuzz.token_set_ratio(text_query, str(t))/100.0
    )
    cand["final_score"] = 0.9*cand["score_vec"] + 0.1*cand["keyword_boost"]
    out = cand.sort_values("final_score", ascending=False).head(k)[["keyword","final_score"]]
    return out.reset_index(drop=True)

def search_by_article_file(path: Path, k: int = 10, openai_key: str = ""):
    text = Path(path).read_text(encoding="utf-8")
    client = OpenAI(api_key=openai_key)
    idx, meta = load_index_and_meta()

    qv = embed_one(client, clean_text(text))
    topn = min(max(k*5, k), len(meta))
    D, I = idx.search(qv, topn)

    cand = meta.iloc[I[0]].copy()
    cand["score_vec"] = D[0]
    cand["keyword_boost"] = cand["keyword"].fillna("").apply(
        lambda t: fuzz.token_set_ratio(text, str(t))/100.0
    )
    cand["final_score"] = 0.9*cand["score_vec"] + 0.1*cand["keyword_boost"]
    out = cand.sort_values("final_score", ascending=False).head(k)[["keyword","final_score"]]
    return out.reset_index(drop=True)

# ================== APP FLOW ==================
def main():
    # 1) Î–Î®Ï„Î± 2 keys + CSV ÏƒÏ„Î·Î½ Î±ÏÏ‡Î®
    openai_key, other_key, csv_path, keywords = interactive_collect_credentials_and_csv()

    # 2) Î§Ï„Î¯ÏƒÎµ index (Ï€Î¬Î½Ï„Î± Î® ÏÏÏ„Î± Î±Î½ Î¸ÎµÏ‚ full control)
    if INDEX_PATH.exists() or META_PATH.exists():
        if ask_yesno("Î¥Ï€Î¬ÏÏ‡ÎµÎ¹ Î®Î´Î· index. Î˜ÎµÏ‚ Î½Î± Ï„Î¿Î½ Î¾Î±Î½Î±Ï‡Ï„Î¯ÏƒÏ‰ Î±Ï€ÏŒ Ï„Î¿ CSV;", default_yes=False):
            build_index_from_keywords(keywords, openai_key)
    else:
        build_index_from_keywords(keywords, openai_key)

    # 3) Loop Î¼ÎµÎ½Î¿Ï
    while True:
        print("\n=== ÎœÎµÎ½Î¿Ï ===")
        print("1) Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î¼Îµ Î¼Î¹ÎºÏÎ® Ï†ÏÎ¬ÏƒÎ·")
        print("2) Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î¼Îµ Ï€Î»Î®ÏÎµÏ‚ Î¬ÏÎ¸ÏÎ¿ (.txt)")
        print("3) ÎˆÎ¾Î¿Î´Î¿Ï‚")
        choice = input("Î•Ï€Î¹Î»Î¿Î³Î® [1/2/3]: ").strip()

        if choice == "1":
            q = input("Î“ÏÎ¬ÏˆÎµ Ï„Î¿ query: ").strip()
            if not q:
                print("ÎšÎµÎ½ÏŒ query.")
                continue
            try:
                res = search_by_query(q, k=10, openai_key=openai_key)
                print("\nÎ‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±:")
                print(res.to_string(index=False))
            except Exception as e:
                print(f"Î£Ï†Î¬Î»Î¼Î±: {e}")

        elif choice == "2":
            p = input("Î”ÏÏƒÎµ Î´Î¹Î±Î´ÏÎ¿Î¼Î® .txt Î¼Îµ Ï„Î¿ Î¬ÏÎ¸ÏÎ¿: ").strip().strip('"').strip("'")
            if not p:
                print("ÎšÎµÎ½ÏŒ path.")
                continue
            try:
                res = search_by_article_file(Path(p), k=10, openai_key=openai_key)
                print("\Î½Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±:")
                print(res.to_string(index=False))
            except Exception as e:
                print(f"Î£Ï†Î¬Î»Î¼Î±: {e}")

        elif choice == "3":
            print("ÎˆÎ¾Î¿Î´Î¿Ï‚. ğŸ‘‹")
            break
        else:
            print("ÎœÎ· Î­Î³ÎºÏ…ÏÎ· ÎµÏ€Î¹Î»Î¿Î³Î®. Î”Î¹Î¬Î»ÎµÎ¾Îµ 1/2/3.")

if __name__ == "__main__":
    main()